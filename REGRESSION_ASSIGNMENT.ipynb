{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.**What is Simple Linear Regression\n"
      ],
      "metadata": {
        "id": "OivV_KeYwO2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  SLR attempts to determine the strength and characetristics of the realtion between one independent variable(x) and another dependent variable (y)"
      ],
      "metadata": {
        "id": "-mGvWSUzwl9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.**What are the key assumptions of Simple Linear Regression"
      ],
      "metadata": {
        "id": "NciyrhIrwmjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  It is a statistical method that is used for predictive analysis.\n",
        "-  Linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc."
      ],
      "metadata": {
        "id": "pbzBxjEuw0lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.**What does the coefficient m represent in the equation Y=mX+c"
      ],
      "metadata": {
        "id": "B6DO9g6Gw1UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- y = mx + c refers to an equation of a line having a gradient of m and a y-intercept of c. This equation is often referred to as the slope-intercept form of the equation of a line. This equation also forms an important equation in the new science of artificial intelligence, to help predict the values, based on the input variable values."
      ],
      "metadata": {
        "id": "2t10qfGuw9AZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.** What does the intercept c represent in the equation Y=mX+c"
      ],
      "metadata": {
        "id": "7J7qsLG-w9nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Intercept: In this equation, the value 'c' is called the intercept of the line. The intercept measures the length where the line cuts the y-axis, from the origin. It can also be interpreted as the point (0, c) on the y-axis, through which the line is passing."
      ],
      "metadata": {
        "id": "XdIryZSGxGf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.**How do we calculate the slope m in Simple Linear Regression"
      ],
      "metadata": {
        "id": "Wj6V-jezxG8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The slope in a regression equation can be found by calculating the coefficient associated with the independent variable(s) in the regression model."
      ],
      "metadata": {
        "id": "usnGsNZlxNW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.**What is the purpose of the least squares method in Simple Linear Regression"
      ],
      "metadata": {
        "id": "EsNzu1v5xN6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Least Square method is a fundamental mathematical technique widely used in data analysis, statistics, and regression modeling to identify the best-fitting curve or line for a given set of data points. This method ensures that the overall error is reduced, providing a highly accurate model for predicting future data trends"
      ],
      "metadata": {
        "id": "eK8H3OrexUmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.** How is the coefficient of determination (R²) interpreted in Simple Linear Regression"
      ],
      "metadata": {
        "id": "rlxakWZExVIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The coefficient of determination (R²) measures how well a statistical model predicts an outcome. The outcome is represented by the model’s dependent variable.\n",
        "\n",
        " The lowest possible value of R² is 0 and the highest possible value is 1. Put simply, the better a model is at making predictions, the closer its R² will be to 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "uizjIdHaxa-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.**What is Multiple Linear Regression"
      ],
      "metadata": {
        "id": "1MnTV9ZjxbVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable."
      ],
      "metadata": {
        "id": "l8Gh8xQ4xfn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.** What is the main difference between Simple and Multiple Linear Regression"
      ],
      "metadata": {
        "id": "gg_6ferVxgAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Simple Linear Regression:-In Simple Linear Regression, there is only one independent variable used to predict the dependent variable. The relationship between the independent and dependent variables is assumed to be linear, meaning that a change in the independent variable will result in a proportional change in the dependent variable.\n",
        "\n",
        "-  Multiple Linear Regression:-Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable."
      ],
      "metadata": {
        "id": "fJ1f4GgJxlPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.** What are the key assumptions of Multiple Linear Regression"
      ],
      "metadata": {
        "id": "_c9IHMO4xlpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Homogeneity of variance (homoscedasticity): the size of the error in our prediction doesn’t change significantly across the values of the independent variable.\n",
        "\n",
        "-  Independence of observations: the observations in the dataset were collected using statistically valid sampling methods, and there are no hidden relationships among variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "xBD1F227xqnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.** What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression mode"
      ],
      "metadata": {
        "id": "jn2zhuoOxq96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  We can define heteroscedasticity as the condition in which the variance of error term or the residual term in a regression model varies.When heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust. Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn’t pick up on this.\n",
        "\n"
      ],
      "metadata": {
        "id": "QKy009vgxwKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.** How can you improve a Multiple Linear Regression model with high multicollinearity"
      ],
      "metadata": {
        "id": "6zhABoOVxwkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Multicollinearity occurs when two or more independent variables in a linear regression model are highly correlated.\n",
        "-  Increase the sample size: to improve model accuracy, making it easier to differentiate between the effects of different predictors.\n",
        "-  Combine correlated variables and combine them into a single, more meaningful predictor. This can be done using techniques like Principal Component Analysis (PCA) or factor analysis, which help reduce redundancy by creating a new variable that represents the combined information.\n"
      ],
      "metadata": {
        "id": "9fE4Nmu-x10Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.** What are some common techniques for transforming categorical variables for use in regression models"
      ],
      "metadata": {
        "id": "q8Yk_NAJx2KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Transforming categorical variables is a key step in preparing data for regression models, especially because these models require numerical input. Here are some of the most common techniques used:\n",
        "\n",
        "-  1. One-Hot Encoding:-Converts each category into a separate binary column (0 or 1).\n",
        "\n",
        "  It is used When the categorical variable is nominal (no intrinsic order), and the number of categories is not too large.\n",
        "\n",
        "-  2. Label Encoding:-Assigns each category a unique integer.\n",
        "It is used Sometimes okay for ordinal variables (like \"Low\", \"Medium\", \"High\").\n"
      ],
      "metadata": {
        "id": "icHFUlKDx9qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.**What is the role of interaction terms in Multiple Linear Regression"
      ],
      "metadata": {
        "id": "hnzfFT2cx-Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Interaction terms play a key role in enhancing the explanatory power of multiple linear regression models—especially when the effect of one variable depends on the level of another.\n",
        "Interaction terms are created by multiplying two or more predictor variables together. They capture the combined effect of variables on the target variable that goes beyond their individual effects.\n",
        "\n"
      ],
      "metadata": {
        "id": "UoRsHmHXyDdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.** How can the interpretation of intercept differ between Simple and Multiple Linear Regression"
      ],
      "metadata": {
        "id": "pTG79m-2yDzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The intercept (often denoted as β₀) plays a slightly different interpretive role in Simple Linear Regression (SLR) versus Multiple Linear Regression (MLR)—even though mathematically, it's doing the same thing: providing a baseline value for the predicted outcome when all predictors are zero."
      ],
      "metadata": {
        "id": "R2HarRRWyLvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.**What is the significance of the slope in regression analysis, and how does it affect predictions"
      ],
      "metadata": {
        "id": "vrGSEwP5yMJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A regression slope is a measure of the relationship between two variables in a linear regression model. It represents the change in the dependent variable for a unit change in the independent variable. The significance of a regression slope lies in its ability to provide valuable insights into the strength and direction of the relationship between the variables."
      ],
      "metadata": {
        "id": "RWSr825OyTjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.** How does the intercept in a regression model provide context for the relationship between variables"
      ],
      "metadata": {
        "id": "rUQYdjnZyT6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The coefficients in the equation define the relationship between each independent variable and the dependent variable. The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero.\n"
      ],
      "metadata": {
        "id": "038uSkvcydBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.**What are the limitations of using R² as a sole measure of model performance"
      ],
      "metadata": {
        "id": "vB2VMmaiydZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  While R² provides valuable insights into model performance, it has limitations. It can be misleading for models that overfit the data, as a high R² might indicate that the model is too complex and captures noise rather than true patterns.\n"
      ],
      "metadata": {
        "id": "g72KOWvDyiiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.**How would you interpret a large standard error for a regression coefficient"
      ],
      "metadata": {
        "id": "cl386iW_yi94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  It measures the variability (uncertainty) of your estimated coefficient across repeated samples.\n",
        "\n",
        "-  The estimate of the coefficient is imprecise.\n",
        "\n",
        "- There’s a lot of uncertainty about the true effect of the variable.\n",
        "\n",
        "-  The coefficient may not be statistically significant (t-statistic will be small → high p-value).\n",
        "\n"
      ],
      "metadata": {
        "id": "3vFIo8E6yox9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.**How can heteroscedasticity be identified in residual plots, and why is it important to address it"
      ],
      "metadata": {
        "id": "Jf1HYEInypJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  It means the variance of the residuals (errors) isn’t constant across all levels of the independent variable(s).\n",
        "ransform the Dependent Variable\n",
        "\n",
        "Log, square root, or Box-Cox transformation often stabilizes variance.\n",
        "\n",
        "Example:\n",
        "If your model is predicting income, try log(income) instead of income.\n",
        "\n",
        "Use Robust Standard Errors\n",
        "\n",
        "These adjust for heteroscedasticity without changing your coefficients.\n",
        "\n",
        "Often available as HC0, HC3, etc. in statistical packages.\n",
        "\n",
        "Weighted Least Squares (WLS)\n",
        "\n",
        "Give less weight to observations with higher variance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "17al5N-0ytwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21.** What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²"
      ],
      "metadata": {
        "id": "yvcZf2uqyuHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A high R² and low adjusted R² in multiple linear regression suggests that the model might be overfitting, with added predictors not significantly improving predictive power. This indicates that the model's performance on new data might be lower than suggested by the high R² value."
      ],
      "metadata": {
        "id": "Ypb8y57Ny2h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22.**Why is it important to scale variables in Multiple Linear Regression"
      ],
      "metadata": {
        "id": "nj1SjKoFy27j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Scaling variables in Multiple Linear Regression is important because it improves the model's performance and interpretability, especially when dealing with variables on significantly different scales. By scaling, the model can converge faster and more accurately, and the resulting coefficients become easier to compare and understand."
      ],
      "metadata": {
        "id": "-bWrvkj7y74A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23.**What is polynomial regression"
      ],
      "metadata": {
        "id": "vCwZiuPiy8PY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as a polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x)."
      ],
      "metadata": {
        "id": "4wmOAXyPzATe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24.** How does polynomial regression differ from linear regression"
      ],
      "metadata": {
        "id": "J95bt79KzAud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. Unlike linear regression, polynomial regression can fit non-linear relationships between variables."
      ],
      "metadata": {
        "id": "TPp_fsiPzHdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25.** When is polynomial regression used"
      ],
      "metadata": {
        "id": "GLKAadyHzH5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Polynomial regression is used when the relationship between independent and dependent variables is not linear, but rather exhibits a curvilinear pattern."
      ],
      "metadata": {
        "id": "773flkFJzOPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q26.** What is the general equation for polynomial regression"
      ],
      "metadata": {
        "id": "dS6ds0GzzOqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  y=f(x)=β0+β1x+β2x2+β3x3+… +βdxd+ + β d x d + ϵ"
      ],
      "metadata": {
        "id": "iX3mT8_czUFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q27.**What are the limitations of polynomial regression"
      ],
      "metadata": {
        "id": "l1Xu6F0kzUjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Limitations of Polynomial Regression\n",
        "-  Overfitting: Higher-degree polynomial models are susceptible to overfitting, where the model fits the training data too closely and loses generalization ability. Careful model selection and regularization techniques are required to mitigate this risk.\n",
        "-  Computational Complexity: As the degree of the polynomial increases, so does the computational complexity. Extremely high-degree polynomials can be computationally expensive and may not provide meaningful improvements in model performance.\n",
        "-  Data Requirements: Polynomial regression works best when you have a sufficient amount of data, especially for high-degree polynomials. Small datasets may not provide enough information to estimate the complex polynomial terms reliably.\n"
      ],
      "metadata": {
        "id": "QV8mIcgwzfoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q28.**What methods can be used to evaluate model fit when selecting the degree of a polynomial"
      ],
      "metadata": {
        "id": "PAFyklfTzgA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Several methods can be used to evaluate model fit when selecting the degree of a polynomial, including cross-validation, visual inspection, and goodness-of-fit statistics. Cross-validation helps assess how well the model generalizes to unseen data by splitting the data into training and validation sets."
      ],
      "metadata": {
        "id": "xm26v6dzzpwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q29.**Can polynomial regression be applied to multiple variables"
      ],
      "metadata": {
        "id": "WVFanyi6zqRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Yes, polynomial regression can be applied to multiple variables, a technique often referred to as multivariate polynomial regression. It allows for modeling complex, non-linear relationships between multiple independent variables and a dependent variable."
      ],
      "metadata": {
        "id": "nrSLlFFa0CfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q30.**Why is visualization important in polynomial regression"
      ],
      "metadata": {
        "id": "cK0IbnHv0C4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Visualization is crucial in polynomial regression because it helps understand the data's structure, model performance, and potential issues like overfitting or underfitting. By visualizing the data and the regression model, you can assess how well the model fits the data, identify non-linear patterns, and make informed decisions about the polynomial degree."
      ],
      "metadata": {
        "id": "cROXusEL0Iz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q31.** How is polynomial regression implemented in Python"
      ],
      "metadata": {
        "id": "b8iUK-420JMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "datas = pd.read_csv('data.csv')\n",
        "datas"
      ],
      "metadata": {
        "id": "YL5g-Teq8XUA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}